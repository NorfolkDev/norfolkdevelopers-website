---
layout: post
title: "Web Scraping static web pages in .Net Core"
date: "2022-05-18T22:45:00Z"
author: Christopher Long
tags: Web Scraping, .Net Core, C#
---

## Overview
I was talking to a developer friend the other day about starting a little side project where you could keep track of the product prices on supermarket websites and trend them over time. This would allow a user to see whatâ€™s increasing in price at any given moment. Unfortunately, none of the major supermarkets offer convenient APIâ€™s to easily get this information. Tescoâ€™s used to offer a handy API, but it was decommissioned in 2016, sad face.
This left me with a slightly less elegant method of getting the data I wanted, web scraping. This is something I have very little experience in, so itâ€™s time to start at the basics.

## What is Web Scraping?
Simply put, web scraping is your program using a regular old HTTP request to go off to a website of your choosing and grab all the HTML/CSS from the page. Usually, youâ€™d then want to parse the results, only saving the data youâ€™re interested in.
You need to use different methods of scraping for different types of web pages. To start with weâ€™re going to look at how to get data from static web pages.

## Prerequisites
Iâ€™m going to be building this in a .Net Core Web Application using MVC. So, if youâ€™re following along, go ahead and create a new project.
![Creating a new .net project.](/static/images/pages/posts/2022-05-18-scraping-static-web-pages/createNewProject.png)
 
Iâ€™ll also be making use of the â€œHTML Agility Packâ€ which you can find via the NuGet package manager:
![Adding the Agility Pack to the solution using NuGet.](/static/images/pages/posts/2022-05-18-scraping-static-web-pages/getAgilityPack.png)
This package makes parsing the HTML content much more intuitive.

## Retrieving the HTML
Fortunately for us, .Net Core includes native asynchronous HTTP request libraries. Lets get us some HTML!
First step is choosing the web page we want to scrape, as I mentioned weâ€™re starting with static pages, so Iâ€™ve chosen a Wikipedia page dear to all our hearts.
![Setting the URL var](/static/images/pages/posts/2022-05-18-scraping-static-web-pages/SetURL.png)
 
As you can see above Iâ€™ve placed the URL variable in the HomeController Index() method as this is the default call when you first open an MVC web application.
When using the .NET HTTP library, a static asynchronous task is returned from the request, so weâ€™ll need to build out the code to handle the request functionality in its own static method.
![Building out the code](/static/images/pages/posts/2022-05-18-scraping-static-web-pages/startBuildingOutTheCode.png)
 
Iâ€™ve added this to the HomeController class to keep things simple. Iâ€™ve also updated the Index() method to call the method on our URL.
![index update](/static/images/pages/posts/2022-05-18-scraping-static-web-pages/call.png)
 
Letâ€™s quickly test that weâ€™ve set things up correctly and are receiving the HTML data. The easiest way to do this is to place a breakpoint on the return View(); line and run the program.
![Checking the HTML](/static/images/pages/posts/2022-05-18-scraping-static-web-pages/CheckingTheHTML.gif)
Looks good! Now to parse the data.

## Parsing the HTML
The first step in parsing the HTML is knowing what weâ€™re looking for. For this exercise Iâ€™m happy with just getting all the individual programming languages held on the page. 
Inspecting the programming names in the chrome dev tools shows us that each name is contained in an <li> element. 
![ğŸ‘€ Inspecting the HTML ğŸ‘€](/static/images/pages/posts/2022-05-18-scraping-static-web-pages/inspecting.png)
 
Letâ€™s grab all the <li>â€™s on the page and see what we get.
First Iâ€™m going to create a new method to parse the HTML.
![Getting all the Li elements](/static/images/pages/posts/2022-05-18-scraping-static-web-pages/gimmeSomeLis.png)
 
While writing the method I used breakpoints to see the structure of the list items, the InnerText property appears to contain the data I want.
![Getting the correct data from the innerText property](/static/images/pages/posts/2022-05-18-scraping-static-web-pages/innerText.png)
 
The problem with this approach is that there are a lot of other <li> elements on the page and Iâ€™m getting their values as well, like this.
![Get these Li's out of my face](/static/images/pages/posts/2022-05-18-scraping-static-web-pages/thatsALotOfLis.png)
 
By poking around in the HTML source I was able to get values from the elements, or their parent nodes that I could start using to fiter out some of the unwanted data.
![Filtering the results](/static/images/pages/posts/2022-05-18-scraping-static-web-pages/filtering.png)
 
I didnâ€™t sanitize the whole list since I donâ€™t really have any plans for the data here, but you can see the method I used and see how that could be incorporated in your own code.

## Saving the filtered data
It may be more convenient in your project to just save the data and do the sanitizing some other way after the fact. Since weâ€™re keeping things simple lets just write a quick method to export this data to a .txt file.
![Saving this ğŸ’©](/static/images/pages/posts/2022-05-18-scraping-static-web-pages/saving.png)

And boom, hereâ€™s out output:
![output](/static/images/pages/posts/2022-05-18-scraping-static-web-pages/output.png)
 
## Conclusion
Web scraping is one of the various methods available to a developer who needs to make use of a web pageâ€™s content. In this article I covered a basic but powerful implementation, I hope to also cover scraping dynamically created webpages in the future.

Cheers!
